'''
This script takes a summary file + graph file directory and outputs data needed
to screen for hotspots.

The summary file is generated by first running the aggregate.csv script.
'''
import argparse
import os
import networkx as nx
import utilities as utils
import graph_utilities as gutils
import graph_generators as ggens
import matplotlib.pyplot as plt
import statistics as stats


# TODO - handle well-mixed and toroidal lattice separately
exclude_graphs = {
    # "clique-ring_",
    # "comet-kite_",
    # "well-mixed"
}


def task_name(in_name):
    '''
    Annoyingly, task names are inconsistent because of avida oddities
    '''
    mapping = {"andnot": "andn", "ornot": "orn", "equals":"equ"}
    if in_name in mapping:
        return mapping[in_name]
    elif "logic_" in in_name:
        parts = in_name.split("_")
        parts[-1] = parts[-1].upper()
        return "_".join(parts)
    else:
        return in_name

def calc_graph_summary_properties(graph):
    properties = {}
    # Check connectivity
    is_connected = nx.is_connected(graph)
    print("  analyzing graph summary properties")
    # Density
    properties["density"] = nx.density(graph)
    # Degree (mean, median, variance)
    node_degrees = [pair[1] for pair in nx.degree(graph)]
    properties["degree_mean"] = stats.mean(node_degrees)
    properties["degree_median"] = stats.median(node_degrees)
    properties["degree_variance"] = stats.variance(node_degrees)
    print("  degree done")
    properties["girth"] = nx.girth(graph)
    properties["degree_assortivity_coef"] = nx.degree_assortativity_coefficient(graph)
    properties["num_bridges"] = len(list(nx.bridges(graph)))
    # properties["max_clique_size"] = len(nx.make_max_clique_graph(graph).nodes)
    properties["transitivity"] = nx.transitivity(graph)
    print("  transitivity done")
    # properties["avg_clustering"] = nx.average_clustering(graph)
    properties["num_connected_components"] = nx.number_connected_components(graph)
    # properties["num_articulation_points"] = len(list(nx.articulation_points(graph)))
    properties["avg_node_connectivity"] = nx.average_node_connectivity(graph)
    properties["edge_connectivity"] = nx.edge_connectivity(graph)
    properties["node_connectivity"] = nx.node_connectivity(graph)
    print("  connectivity done")
    properties["diameter"] = nx.diameter(graph) if is_connected else "error"
    properties["radius"] = nx.radius(graph) if is_connected else "error"
    properties["kemeny_constant"] = nx.kemeny_constant(graph) if is_connected else "error"
    print("  kemeny constant done")
    # properties["global_efficiency"] = nx.global_efficiency(graph)
    # properties["wiener_index"] = nx.wiener_index(graph)
    length_results = nx.all_pairs_shortest_path_length(graph)
    max_short_path = max([max(focal_node[1].values()) for focal_node in length_results])
    properties["longest_shortest_path"] = max_short_path
    print("  done analyzing graph summary properties")
    # properties["connectivity"] = nx.all_pairs_node_connectivity(graph)

    return properties

def main():
    parser = argparse.ArgumentParser(description="Screen for hotspots")
    parser.add_argument("--summary_data", type=str, help="Summary data file that contains task appearance data.")
    parser.add_argument("--graph_birth_data", type=str, help="Summary data file containing graph birth location data.")
    parser.add_argument("--graphs_dir", type=str, help="Path to directory containing relevant graphs")
    parser.add_argument("--dump_dir", type=str, default=".", help="Where to write output files")

    args = parser.parse_args()
    summary_data_path = args.summary_data
    birth_locs_data_path = args.graph_birth_data
    graphs_dir = args.graphs_dir
    dump_dir = args.dump_dir

    if not os.path.isfile(summary_data_path):
        print(f"Failed to find summary data file: {summary_data_path}")
        exit(-1)

    if not os.path.isdir(graphs_dir):
        print(f"Failed to find graphs directory: {graphs_dir}")
        exit(-1)

    utils.mkdir_p(dump_dir)

    # Read over summary file
    summary_data = utils.read_csv(summary_data_path)
    # NOTE - assumes all runs in summary have same world size
    world_x = int(summary_data[-1]["WORLD_X"])
    world_y = int(summary_data[-1]["WORLD_Y"])
    world_size = world_x * world_y

    # Collect all tasks
    tasks = [
        field.replace("pop_task_", "")
        for field in summary_data[-1]
        if "pop_task_" in field and field != "pop_task_total"
    ]
    print(tasks)

    ############################################################################
    # Load summary data, extract task first appearance locations
    ############################################################################
    # Organize data by graph file / graph type
    data_by_graph = {}
    for line in summary_data:
        if any(g in line["graph_file"] for g in exclude_graphs):
            continue
        graph = line["graph_file"] if line["graph_file"] != "none" else line["graph_type"]
        if not graph in data_by_graph:
            data_by_graph[graph] = []
        data_by_graph[graph].append(line)

    graph_files = list(data_by_graph.keys())
    graph_files.sort()

    # Distributions of task locations by task
    graph_task_loc_info = {
        graph: { task:[] for task in tasks }
        for graph in graph_files
    }

    graph_tasks_agg_loc_info = {
        graph: []
        for graph in graph_files
    }

    graph_tasks_appearances = {
        graph: { task: 0 for task in tasks }
        for graph in graph_files
    }

    graph_total_tasks = {
        graph: 0
        for graph in graph_files
    }

    # For each graph, get all first location appearances
    for graph in graph_task_loc_info:
        for line in data_by_graph[graph]:
            for task in tasks:
                tname = task_name(task)
                task_completed = (line[f"task_loc_{tname}_completed"] == "1") and not ((line[f"task_loc_{tname}_loc_id"] == "-1"))
                if not task_completed:
                    continue
                # task_loc_x = line[f"task_loc_{task}_loc_x"]
                # task_loc_y = line[f"task_loc_{task}_loc_y"]
                task_loc_id = int(line[f"task_loc_{tname}_loc_id"])
                graph_tasks_agg_loc_info[graph].append(task_loc_id)
                graph_task_loc_info[graph][task].append(task_loc_id)
                graph_tasks_appearances[graph][task] += 1
                graph_total_tasks[graph] += 1


    ############################################################################
    # Add task appearances by location
    ############################################################################
    task_location_info = {}
    for graph in graph_task_loc_info:
        task_location_info[graph] = {}
        # Add per-task appearances to location information
        for task in tasks:
            task_locs = graph_task_loc_info[graph][task]
            for loc in task_locs:
                if not loc in task_location_info[graph]:
                    task_location_info[graph][loc] = {task:{"count":0,"prop":0} for task in tasks}
                    task_location_info[graph][loc]["all"] = {"count":0, "prop":0}
                task_location_info[graph][loc][task]["count"] += 1
                task_location_info[graph][loc]["all"]["count"] += 1

        # Calculate proportions (per task and aggregated)
        for loc in task_location_info[graph]:
            for task in tasks:
                if graph_tasks_appearances[graph][task] > 0:
                    task_location_info[graph][loc][task]["prop"] = task_location_info[graph][loc][task]["count"] / graph_tasks_appearances[graph][task]
            if graph_total_tasks[graph] > 0:
                task_location_info[graph][loc]["all"]["prop"] = task_location_info[graph][loc]["all"]["count"] / graph_total_tasks[graph]

    ############################################################################
    # For each graph, get birth counts by location
    ############################################################################
    birth_loc_data = utils.read_csv(birth_locs_data_path)
    actual_birth_locs = {} # {graph: {loc: [birth counts]}}
    for line in birth_loc_data:
        line_graph = line["graph_file"]
        if not line_graph in actual_birth_locs:
            actual_birth_locs[line_graph] = {}
        loc = int(line["loc_id"])
        if not loc in actual_birth_locs[line_graph]:
            actual_birth_locs[line_graph][loc] = {"counts": []}
        actual_birth_locs[line_graph][loc]["counts"].append(int(line["births"]))

    # Summarize each distribution
    total_births_by_graph = {}
    for graph_file in actual_birth_locs:
        total_births = 0
        for loc in actual_birth_locs[graph_file]:
            loc_info = actual_birth_locs[graph_file][loc]
            loc_info["total"] = sum(loc_info["counts"])
            loc_info["mean"] = stats.mean(loc_info["counts"])
            loc_info["variance"] = stats.variance(loc_info["counts"])
            total_births += loc_info["total"]
        total_births_by_graph[graph_file] = total_births
    # Compute proportions
    for graph_file in actual_birth_locs:
        # print(f"Graph file: {graph_file}")
        for loc in actual_birth_locs[graph_file]:
            loc_info = actual_birth_locs[graph_file][loc]
            loc_info["prop"] = 0
            if total_births_by_graph[graph_file] > 0:
                loc_info["prop"] = loc_info["total"] / total_births_by_graph[graph_file]
            # print(f"Loc {loc}: {loc_info['prop']}")

    ############################################################################
    # Load each graph, output properties
    ############################################################################
    graph_expected_births_info = {}
    graph_summary_info = []
    for graph_file in graph_files:
        print(f"Graph file: {graph_file}")
        # if graph_file == "well-mixed": continue
        graph_path = os.path.join(graphs_dir, graph_file)
        # Load graph
        # - If well-mixed or torroidal lattice generate directly
        # graph = None
        # if graph_file in ["toroidal-lattice", "torroidal-lattice"]:
        #     graph = ggens.gen_graph_toroidal_lattice(world_x, world_y)
        # else:
        graph = gutils.read_graph_matrix(graph_path)

        graph_expected_births_info[graph_file] = gutils.calc_expected_births(graph)

        graph_summary_info.append(calc_graph_summary_properties(graph))


        node_properties = {
            # "percolation_centrality": nx.percolation_centrality(graph),
            # "harmonic_centrality": nx.harmonic_centrality(graph),
            # "subgraph_centrality": nx.subgraph_centrality(graph),
            # "eigenvector_centrality": nx.eigenvector_centrality(graph),
            # "load_centrality": nx.load_centrality(graph),
            # "second_order_centrality": nx.second_order_centrality(graph),
            # "triangles": nx.triangles(graph),
            # "closeness_centrality": nx.closeness_centrality(graph),
            # "information_centrality": nx.information_centrality(graph),
            # "clustering": nx.clustering(graph)
        }

        # Add attributes to graph nodes
        for loc in graph.nodes():
            expected_births = graph_expected_births_info[graph_file][loc]["prop_births"]
            actual_births_prop = actual_birth_locs[graph_file][loc]["prop"]
            all_task_prop = task_location_info[graph_file][loc]["all"]["prop"] if loc in task_location_info[graph_file] else 0
            all_vs_expected = all_task_prop - expected_births
            all_vs_actual = all_task_prop - actual_births_prop
            graph.nodes[loc]["expected_births_prop"] = expected_births
            graph.nodes[loc]["actual_births_prop"] = actual_births_prop
            graph.nodes[loc]["all_task_prop"] = all_task_prop
            graph.nodes[loc]["all_task_prop_vs_expected_births"] = all_vs_expected
            graph.nodes[loc]["all_task_prop_vs_actual_births"] = all_vs_actual
            graph.nodes[loc]["actual_births_mean"] = actual_birth_locs[graph_file][loc]["mean"]
            graph.nodes[loc]["actual_births_variance"] = actual_birth_locs[graph_file][loc]["variance"]
            graph.nodes[loc]["actual_births_stddev"] = stats.stdev(actual_birth_locs[graph_file][loc]["counts"])
            graph.nodes[loc]["degree"] = graph.degree[loc]
            for property in node_properties:
                graph.nodes[loc][property] = node_properties[property][loc]

        # TODO: add more node properties to each node
        # print(nx.degree_centrality(graph))

        # Write out node info for graph
        graph_base_name = ".".join(graph_file.split(".")[:-1])
        graph_output_fname = f"node_info_{graph_base_name}.csv"
        graph_output_fpath = os.path.join(dump_dir, graph_output_fname)
        gutils.write_node_info(graph_output_fpath, graph)

        graph_summary_info[-1]["graph_name"] = graph_base_name
        # Draw graph
        # nx.draw(
        #     graph,
        #     pos = nx.planar_layout(graph)
        #     # pos = nx.spring_layout(graph, iterations=100)
        #     # with_labels = True,
        #     # labels = [color_map[node] if node in color_map else num_clique_rings+1 for node in list(graph.nodes())],
        # )
        # plt.show()

    # print(graph_expected_births_info)
    utils.write_csv("graph_summary_properties.csv", graph_summary_info)


if __name__ == "__main__":
    main()